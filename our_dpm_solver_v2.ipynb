{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class NoiseScheduleVP:\n",
        "    def __init__(self, schedule='discrete', betas=None, alphas_cumprod=None, continuous_beta_0=0.1, continuous_beta_1=20., dtype=torch.float32,):\n",
        "\n",
        "        # Initialize the scheduler with predefined beta or alpha cum prod\n",
        "        self.schedule = schedule\n",
        "\n",
        "        if betas is not None:\n",
        "            log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n",
        "        else:\n",
        "            assert alphas_cumprod is not None\n",
        "            log_alphas = 0.5 * torch.log(alphas_cumprod)\n",
        "        self.T = 1.\n",
        "        self.log_alpha_array = self.numerical_clip_alpha(log_alphas).reshape((1, -1,)).to(dtype=dtype)\n",
        "        self.total_N = self.log_alpha_array.shape[1]\n",
        "        self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1)).to(dtype=dtype)\n",
        "\n",
        "    def numerical_clip_alpha(self, log_alphas, clipped_lambda=-5.1):\n",
        "        #  Clipping for stability\n",
        "        log_sigmas = 0.5 * torch.log(1. - torch.exp(2. * log_alphas))\n",
        "        lambs = log_alphas - log_sigmas\n",
        "        idx = torch.searchsorted(torch.flip(lambs, [0]), clipped_lambda)\n",
        "        if idx > 0:\n",
        "            log_alphas = log_alphas[:-idx]\n",
        "        return log_alphas\n",
        "\n",
        "    def marginal_log_mean_coeff(self, t):\n",
        "        return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n",
        "\n",
        "    def marginal_alpha(self, t):\n",
        "        return torch.exp(self.marginal_log_mean_coeff(t))\n",
        "\n",
        "    def marginal_std(self, t):\n",
        "        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n",
        "\n",
        "    def marginal_lambda(self, t):\n",
        "        log_mean_coeff = self.marginal_log_mean_coeff(t)\n",
        "        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n",
        "        return log_mean_coeff - log_std\n",
        "\n",
        "    def inverse_lambda(self, lamb):\n",
        "        log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n",
        "        t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n",
        "        return t.reshape((-1,))\n",
        "\n",
        "def model_wrapper(model, noise_schedule, model_kwargs={}):\n",
        "\n",
        "    def get_model_input_time(t_continuous):\n",
        "        return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n",
        "\n",
        "    def noise_pred_fn(x, t_continuous, cond=None):\n",
        "        t_input = get_model_input_time(t_continuous)\n",
        "        output = model(x, t_input, **model_kwargs)\n",
        "        return output\n",
        "\n",
        "    def model_fn(x, t_continuous):\n",
        "        return noise_pred_fn(x, t_continuous)\n",
        "\n",
        "    return model_fn\n",
        "\n",
        "\n",
        "class DPM_Solver:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_fn,\n",
        "        noise_schedule,\n",
        "        correcting_x0_fn=None,\n",
        "        correcting_xt_fn=None,\n",
        "        thresholding_max_val=1.,\n",
        "        dynamic_thresholding_ratio=0.995,\n",
        "    ):\n",
        "\n",
        "        self.model = lambda x, t: model_fn(x, t.expand((x.shape[0])))\n",
        "        self.noise_schedule = noise_schedule\n",
        "        self.algorithm_type = 'dpmsolver'\n",
        "        if correcting_x0_fn == \"dynamic_thresholding\":\n",
        "            self.correcting_x0_fn = self.dynamic_thresholding_fn\n",
        "        else:\n",
        "            self.correcting_x0_fn = correcting_x0_fn\n",
        "        self.correcting_xt_fn = correcting_xt_fn\n",
        "        self.dynamic_thresholding_ratio = dynamic_thresholding_ratio\n",
        "        self.thresholding_max_val = thresholding_max_val\n",
        "\n",
        "    def dynamic_thresholding_fn(self, x0, t):\n",
        "        dims = x0.dim()\n",
        "        p = self.dynamic_thresholding_ratio\n",
        "        s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n",
        "        s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n",
        "        x0 = torch.clamp(x0, -s, s) / s\n",
        "        return x0\n",
        "\n",
        "    def noise_prediction_fn(self, x, t):\n",
        "        return self.model(x, t)\n",
        "\n",
        "    def model_fn(self, x, t):\n",
        "        return self.noise_prediction_fn(x, t)\n",
        "\n",
        "    def get_time_steps(self, skip_type, t_T, t_0, N, device):\n",
        "        return torch.linspace(t_T, t_0, N + 1).to(device)\n",
        "\n",
        "\n",
        "    def get_orders_and_timesteps_for_singlestep_solver(self, steps, order, skip_type, t_T, t_0, device):\n",
        "        if order == 3:\n",
        "            K = steps // 3 + 1\n",
        "            if steps % 3 == 0:\n",
        "                orders = [3,] * (K - 2) + [2, 1]\n",
        "            elif steps % 3 == 1:\n",
        "                orders = [3,] * (K - 1) + [1]\n",
        "            else:\n",
        "                orders = [3,] * (K - 1) + [2]\n",
        "        elif order == 2:\n",
        "            if steps % 2 == 0:\n",
        "                K = steps // 2\n",
        "                orders = [2,] * K\n",
        "            else:\n",
        "                K = steps // 2 + 1\n",
        "                orders = [2,] * (K - 1) + [1]\n",
        "        elif order == 1:\n",
        "            K = steps\n",
        "            orders = [1,] * steps\n",
        "        else:\n",
        "            raise ValueError(\"'order' must be '1' or '2' or '3'.\")\n",
        "\n",
        "        timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, steps, device)[torch.cumsum(torch.tensor([0,] + orders), 0).to(device)]\n",
        "        return timesteps_outer, orders\n",
        "\n",
        "    def denoise_to_zero_fn(self, x, s):\n",
        "        return self.data_prediction_fn(x, s)\n",
        "\n",
        "    def dpm_solver_first_update(self, x, s, t, model_s=None, return_intermediate=False):\n",
        "        ns = self.noise_schedule\n",
        "        dims = x.dim()\n",
        "        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\n",
        "        h = lambda_t - lambda_s\n",
        "        log_alpha_s, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(t)\n",
        "        sigma_s, sigma_t = ns.marginal_std(s), ns.marginal_std(t)\n",
        "        alpha_t = torch.exp(log_alpha_t)\n",
        "\n",
        "        phi_1 = torch.expm1(h)\n",
        "        if model_s is None:\n",
        "            model_s = self.model_fn(x, s)\n",
        "        x_t = (\n",
        "            torch.exp(log_alpha_t - log_alpha_s) * x\n",
        "            - (sigma_t * phi_1) * model_s\n",
        "        )\n",
        "        if return_intermediate:\n",
        "            return x_t, {'model_s': model_s}\n",
        "        else:\n",
        "            return x_t\n",
        "\n",
        "    def singlestep_dpm_solver_second_update(self, x, s, t, r1=0.5, model_s=None, return_intermediate=False, solver_type='dpmsolver'):\n",
        "        if r1 is None:\n",
        "            r1 = 0.5\n",
        "        ns = self.noise_schedule\n",
        "        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\n",
        "        h = lambda_t - lambda_s\n",
        "        lambda_s1 = lambda_s + r1 * h\n",
        "        s1 = ns.inverse_lambda(lambda_s1)\n",
        "        log_alpha_s, log_alpha_s1, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(s1), ns.marginal_log_mean_coeff(t)\n",
        "        sigma_s, sigma_s1, sigma_t = ns.marginal_std(s), ns.marginal_std(s1), ns.marginal_std(t)\n",
        "        alpha_s1, alpha_t = torch.exp(log_alpha_s1), torch.exp(log_alpha_t)\n",
        "\n",
        "        phi_11 = torch.expm1(r1 * h)\n",
        "        phi_1 = torch.expm1(h)\n",
        "\n",
        "        if model_s is None:\n",
        "            model_s = self.model_fn(x, s)\n",
        "        x_s1 = (\n",
        "            torch.exp(log_alpha_s1 - log_alpha_s) * x\n",
        "            - (sigma_s1 * phi_11) * model_s\n",
        "        )\n",
        "        model_s1 = self.model_fn(x_s1, s1)\n",
        "\n",
        "        if solver_type == 'dpmsolver':\n",
        "            x_t = (\n",
        "                torch.exp(log_alpha_t - log_alpha_s) * x\n",
        "                - (sigma_t * phi_1) * model_s\n",
        "                - (0.5 / r1) * (sigma_t * phi_1) * (model_s1 - model_s)\n",
        "            )\n",
        "        elif solver_type == 'taylor':\n",
        "            x_t = (\n",
        "                torch.exp(log_alpha_t - log_alpha_s) * x\n",
        "                - (sigma_t * phi_1) * model_s\n",
        "                - (1. / r1) * (sigma_t * (phi_1 / h - 1.)) * (model_s1 - model_s)\n",
        "            )\n",
        "        if return_intermediate:\n",
        "            return x_t, {'model_s': model_s, 'model_s1': model_s1}\n",
        "        else:\n",
        "            return x_t\n",
        "\n",
        "    def singlestep_dpm_solver_third_update(self, x, s, t, r1=1./3., r2=2./3., model_s=None, model_s1=None, return_intermediate=False, solver_type='dpmsolver'):\n",
        "        if r1 is None:\n",
        "            r1 = 1. / 3.\n",
        "        if r2 is None:\n",
        "            r2 = 2. / 3.\n",
        "        ns = self.noise_schedule\n",
        "        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\n",
        "        h = lambda_t - lambda_s\n",
        "        lambda_s1 = lambda_s + r1 * h\n",
        "        lambda_s2 = lambda_s + r2 * h\n",
        "        s1 = ns.inverse_lambda(lambda_s1)\n",
        "        s2 = ns.inverse_lambda(lambda_s2)\n",
        "        log_alpha_s, log_alpha_s1, log_alpha_s2, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(s1), ns.marginal_log_mean_coeff(s2), ns.marginal_log_mean_coeff(t)\n",
        "        sigma_s, sigma_s1, sigma_s2, sigma_t = ns.marginal_std(s), ns.marginal_std(s1), ns.marginal_std(s2), ns.marginal_std(t)\n",
        "        alpha_s1, alpha_s2, alpha_t = torch.exp(log_alpha_s1), torch.exp(log_alpha_s2), torch.exp(log_alpha_t)\n",
        "\n",
        "        phi_11 = torch.expm1(r1 * h)\n",
        "        phi_12 = torch.expm1(r2 * h)\n",
        "        phi_1 = torch.expm1(h)\n",
        "        phi_22 = torch.expm1(r2 * h) / (r2 * h) - 1.\n",
        "        phi_2 = phi_1 / h - 1.\n",
        "        phi_3 = phi_2 / h - 0.5\n",
        "\n",
        "        if model_s is None:\n",
        "            model_s = self.model_fn(x, s)\n",
        "        if model_s1 is None:\n",
        "            x_s1 = (\n",
        "                (torch.exp(log_alpha_s1 - log_alpha_s)) * x\n",
        "                - (sigma_s1 * phi_11) * model_s\n",
        "            )\n",
        "            model_s1 = self.model_fn(x_s1, s1)\n",
        "        x_s2 = (\n",
        "            (torch.exp(log_alpha_s2 - log_alpha_s)) * x\n",
        "            - (sigma_s2 * phi_12) * model_s\n",
        "            - r2 / r1 * (sigma_s2 * phi_22) * (model_s1 - model_s)\n",
        "        )\n",
        "        model_s2 = self.model_fn(x_s2, s2)\n",
        "        if solver_type == 'dpmsolver':\n",
        "            x_t = (\n",
        "                (torch.exp(log_alpha_t - log_alpha_s)) * x\n",
        "                - (sigma_t * phi_1) * model_s\n",
        "                - (1. / r2) * (sigma_t * phi_2) * (model_s2 - model_s)\n",
        "            )\n",
        "        elif solver_type == 'taylor':\n",
        "            D1_0 = (1. / r1) * (model_s1 - model_s)\n",
        "            D1_1 = (1. / r2) * (model_s2 - model_s)\n",
        "            D1 = (r2 * D1_0 - r1 * D1_1) / (r2 - r1)\n",
        "            D2 = 2. * (D1_1 - D1_0) / (r2 - r1)\n",
        "            x_t = (\n",
        "                (torch.exp(log_alpha_t - log_alpha_s)) * x\n",
        "                - (sigma_t * phi_1) * model_s\n",
        "                - (sigma_t * phi_2) * D1\n",
        "                - (sigma_t * phi_3) * D2\n",
        "            )\n",
        "\n",
        "        if return_intermediate:\n",
        "            return x_t, {'model_s': model_s, 'model_s1': model_s1, 'model_s2': model_s2}\n",
        "        else:\n",
        "            return x_t\n",
        "\n",
        "    def singlestep_dpm_solver_update(self, x, s, t, order, return_intermediate=False, solver_type='dpmsolver', r1=None, r2=None):\n",
        "        if order == 1:\n",
        "            return self.dpm_solver_first_update(x, s, t, return_intermediate=return_intermediate)\n",
        "        elif order == 2:\n",
        "            return self.singlestep_dpm_solver_second_update(x, s, t, return_intermediate=return_intermediate, solver_type=solver_type, r1=r1)\n",
        "        elif order == 3:\n",
        "            return self.singlestep_dpm_solver_third_update(x, s, t, return_intermediate=return_intermediate, solver_type=solver_type, r1=r1, r2=r2)\n",
        "        else:\n",
        "            raise ValueError(\"Solver order must be 1 or 2 or 3, got {}\".format(order))\n",
        "\n",
        "\n",
        "    def dpm_solver_adaptive(self, x, order, t_T, t_0, h_init=0.05, atol=0.0078, rtol=0.05, theta=0.9, t_err=1e-5, solver_type='dpmsolver'):\n",
        "        ns = self.noise_schedule\n",
        "        s = t_T * torch.ones((1,)).to(x)\n",
        "        lambda_s = ns.marginal_lambda(s)\n",
        "        lambda_0 = ns.marginal_lambda(t_0 * torch.ones_like(s).to(x))\n",
        "        h = h_init * torch.ones_like(s).to(x)\n",
        "        x_prev = x\n",
        "        nfe = 0\n",
        "        if order == 2:\n",
        "            r1 = 0.5\n",
        "            lower_update = lambda x, s, t: self.dpm_solver_first_update(x, s, t, return_intermediate=True)\n",
        "            higher_update = lambda x, s, t, **kwargs: self.singlestep_dpm_solver_second_update(x, s, t, r1=r1, solver_type=solver_type, **kwargs)\n",
        "        elif order == 3:\n",
        "            r1, r2 = 1. / 3., 2. / 3.\n",
        "            lower_update = lambda x, s, t: self.singlestep_dpm_solver_second_update(x, s, t, r1=r1, return_intermediate=True, solver_type=solver_type)\n",
        "            higher_update = lambda x, s, t, **kwargs: self.singlestep_dpm_solver_third_update(x, s, t, r1=r1, r2=r2, solver_type=solver_type, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"For adaptive step size solver, order must be 2 or 3, got {}\".format(order))\n",
        "        while torch.abs((s - t_0)).mean() > t_err:\n",
        "            t = ns.inverse_lambda(lambda_s + h)\n",
        "            x_lower, lower_noise_kwargs = lower_update(x, s, t)\n",
        "            x_higher = higher_update(x, s, t, **lower_noise_kwargs)\n",
        "            delta = torch.max(torch.ones_like(x).to(x) * atol, rtol * torch.max(torch.abs(x_lower), torch.abs(x_prev)))\n",
        "            norm_fn = lambda v: torch.sqrt(torch.square(v.reshape((v.shape[0], -1))).mean(dim=-1, keepdim=True))\n",
        "            E = norm_fn((x_higher - x_lower) / delta).max()\n",
        "            if torch.all(E <= 1.):\n",
        "                x = x_higher\n",
        "                s = t\n",
        "                x_prev = x_lower\n",
        "                lambda_s = ns.marginal_lambda(s)\n",
        "            h = torch.min(theta * h * torch.float_power(E, -1. / order).float(), lambda_0 - lambda_s)\n",
        "            nfe += order\n",
        "        print('adaptive solver nfe', nfe)\n",
        "        return x\n",
        "\n",
        "    def add_noise(self, x, t, noise=None):\n",
        "        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\n",
        "        if noise is None:\n",
        "            noise = torch.randn((t.shape[0], *x.shape), device=x.device)\n",
        "        x = x.reshape((-1, *x.shape))\n",
        "        xt = expand_dims(alpha_t, x.dim()) * x + expand_dims(sigma_t, x.dim()) * noise\n",
        "        if t.shape[0] == 1:\n",
        "            return xt.squeeze(0)\n",
        "        else:\n",
        "            return xt\n",
        "\n",
        "    def inverse(self, x, steps=20, t_start=None, t_end=None, order=2, skip_type='time_uniform',\n",
        "        method='multistep', lower_order_final=True, denoise_to_zero=False, solver_type='dpmsolver',\n",
        "        atol=0.0078, rtol=0.05, return_intermediate=False,\n",
        "    ):\n",
        "        t_0 = 1. / self.noise_schedule.total_N if t_start is None else t_start\n",
        "        t_T = self.noise_schedule.T if t_end is None else t_end\n",
        "        assert t_0 > 0 and t_T > 0, \"Time range needs to be greater than 0. For discrete-time DPMs, it needs to be in [1 / N, 1], where N is the length of betas array\"\n",
        "        return self.sample(x, steps=steps, t_start=t_0, t_end=t_T, order=order, skip_type=skip_type,\n",
        "            method=method, lower_order_final=lower_order_final, denoise_to_zero=denoise_to_zero, solver_type=solver_type,\n",
        "            atol=atol, rtol=rtol, return_intermediate=return_intermediate)\n",
        "\n",
        "    def sample(self, x, steps=20, t_start=None, t_end=None, order=2, skip_type='time_uniform',\n",
        "        method='singlestep', lower_order_final=True, denoise_to_zero=False, solver_type='dpmsolver',\n",
        "        atol=0.0078, rtol=0.05, return_intermediate=False,\n",
        "    ):\n",
        "        t_0 = 1. / self.noise_schedule.total_N if t_end is None else t_end\n",
        "        t_T = self.noise_schedule.T if t_start is None else t_start\n",
        "        assert t_0 > 0 and t_T > 0, \"Time range needs to be greater than 0. For discrete-time DPMs, it needs to be in [1 / N, 1], where N is the length of betas array\"\n",
        "\n",
        "        device = x.device\n",
        "        intermediates = []\n",
        "        with torch.no_grad():\n",
        "\n",
        "            timesteps_outer, orders = self.get_orders_and_timesteps_for_singlestep_solver(steps=steps, order=order, skip_type=skip_type, t_T=t_T, t_0=t_0, device=device)\n",
        "\n",
        "            for step, order in enumerate(orders):\n",
        "                s, t = timesteps_outer[step], timesteps_outer[step + 1]\n",
        "                timesteps_inner = self.get_time_steps(skip_type=skip_type, t_T=s.item(), t_0=t.item(), N=order, device=device)\n",
        "                lambda_inner = self.noise_schedule.marginal_lambda(timesteps_inner)\n",
        "                h = lambda_inner[-1] - lambda_inner[0]\n",
        "                r1 = None if order <= 1 else (lambda_inner[1] - lambda_inner[0]) / h\n",
        "                r2 = None if order <= 2 else (lambda_inner[2] - lambda_inner[0]) / h\n",
        "                x = self.singlestep_dpm_solver_update(x, s, t, order, solver_type=solver_type, r1=r1, r2=r2)\n",
        "                if self.correcting_xt_fn is not None:\n",
        "                    x = self.correcting_xt_fn(x, t, step)\n",
        "                if return_intermediate:\n",
        "                    intermediates.append(x)\n",
        "\n",
        "            if denoise_to_zero:\n",
        "                t = torch.ones((1,)).to(device) * t_0\n",
        "                x = self.denoise_to_zero_fn(x, t)\n",
        "                if self.correcting_xt_fn is not None:\n",
        "                    x = self.correcting_xt_fn(x, t, step + 1)\n",
        "                if return_intermediate:\n",
        "                    intermediates.append(x)\n",
        "        if return_intermediate:\n",
        "            return x, intermediates\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "\n",
        "#############################################################\n",
        "# other utility functions\n",
        "#############################################################\n",
        "\n",
        "def interpolate_fn(x, xp, yp):\n",
        "    \"\"\"\n",
        "    A piecewise linear function y = f(x), using xp and yp as keypoints.\n",
        "    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n",
        "    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n",
        "\n",
        "    Args:\n",
        "        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n",
        "        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n",
        "        yp: PyTorch tensor with shape [C, K].\n",
        "    Returns:\n",
        "        The function values f(x), with shape [N, C].\n",
        "    \"\"\"\n",
        "    N, K = x.shape[0], xp.shape[1]\n",
        "    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)\n",
        "    sorted_all_x, x_indices = torch.sort(all_x, dim=2)\n",
        "    x_idx = torch.argmin(x_indices, dim=2)\n",
        "    cand_start_idx = x_idx - 1\n",
        "    start_idx = torch.where(\n",
        "        torch.eq(x_idx, 0),\n",
        "        torch.tensor(1, device=x.device),\n",
        "        torch.where(\n",
        "            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n",
        "        ),\n",
        "    )\n",
        "    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)\n",
        "    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)\n",
        "    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)\n",
        "    start_idx2 = torch.where(\n",
        "        torch.eq(x_idx, 0),\n",
        "        torch.tensor(0, device=x.device),\n",
        "        torch.where(\n",
        "            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n",
        "        ),\n",
        "    )\n",
        "    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)\n",
        "    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)\n",
        "    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)\n",
        "    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)\n",
        "    return cand\n",
        "\n",
        "\n",
        "def expand_dims(v, dims):\n",
        "    \"\"\"\n",
        "    Expand the tensor `v` to the dim `dims`.\n",
        "\n",
        "    Args:\n",
        "        `v`: a PyTorch tensor with shape [N].\n",
        "        `dim`: a `int`.\n",
        "    Returns:\n",
        "        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n",
        "    \"\"\"\n",
        "    return v[(...,) + (None,)*(dims - 1)]"
      ],
      "metadata": {
        "id": "Slw5jFVfL1d1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}